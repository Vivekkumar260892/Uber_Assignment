{"cells":[{"cell_type":"markdown","source":["Below command can be used to unmount your s3 bucket"],"metadata":{}},{"cell_type":"code","source":["##dbutils.fs.unmount(\"/mnt/s3data\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["You need to run this query only the first time to mount the s3 buckets. Once s3 buckets are mounted, you can comment this block."],"metadata":{}},{"cell_type":"code","source":["# import urllib\n# ACCESS_KEY=\"AKIAVPNAKGPOLMQLIZXN\"\n# SECRET_KEY=\"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\"\n# ENCODED_SECRET_KEY=urllib.parse.quote(SECRET_KEY,\"\")\n# AWS_BUCKET_NAME=\"uber-databricks\"\n# MOUNT_NAME=\"s3data\"\n# dbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY,ENCODED_SECRET_KEY,AWS_BUCKET_NAME),\"/mnt/%s\" % MOUNT_NAME)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/s3data\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/s3data/Uber_Data _3.csv</td><td>Uber_Data _3.csv</td><td>38225</td></tr><tr><td>dbfs:/mnt/s3data/Uber_Data_2.csv</td><td>Uber_Data_2.csv</td><td>38225</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('IMSQL').getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Reading Data and assigning column names"],"metadata":{}},{"cell_type":"code","source":["##df = spark.read.csv(\"/FileStore/tables/Uber_Data.csv\",inferSchema=True,header=False,)\ndf = spark.read.csv(\"dbfs:/mnt/s3data/*.csv\",inferSchema=True,header=False,)\ndf = df.withColumnRenamed(\"_c0\", \"vehicle_id\")\ndf = df.withColumnRenamed(\"_c1\", \"function_id\")\ndf = df.withColumnRenamed(\"_c2\", \"mode\")\ndf = df.withColumnRenamed(\"_c3\", \"epoch\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["df.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+-----+-----------+\nvehicle_id|function_id| mode|      epoch|\n+----------+-----------+-----+-----------+\n        v6|        fn8|start|302.8869272|\n        v6|        fn7|start| 211.154259|\n        v6|        fn8|  end|28.91698094|\n        v6|        fn8|start|103.5352043|\n        v6|        fn8|  end|349.7041439|\n+----------+-----------+-----+-----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Total number of rows in this batch and initial Dataframe"],"metadata":{}},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: 3072</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["Dropping null values to create a new dataframe"],"metadata":{}},{"cell_type":"code","source":["df_logs = df.dropna(subset=('vehicle_id','function_id','mode','epoch'))\n\ndf_logs.count(),df_logs.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+-----+-----------+\nvehicle_id|function_id| mode|      epoch|\n+----------+-----------+-----+-----------+\n        v6|        fn8|start|302.8869272|\n        v6|        fn7|start| 211.154259|\n        v6|        fn8|  end|28.91698094|\n        v6|        fn8|start|103.5352043|\n        v6|        fn8|  end|349.7041439|\n        v6|        fn7|  end| 597.062778|\n        v6|        fn8|  end|374.8058183|\n        v6|        fn7|start|512.9982936|\n        v7|        fn8|  end|3.362735724|\n        v6|        fn8|start| 90.5105228|\n        v6|        fn7|  end|507.2910144|\n        v6|        fn7|start|62.23687433|\n        v7|        fn7|start|106.2874892|\n        v6|        fn8|start|263.9960028|\n        v6|        fn8|start|273.5287378|\n        v8|        fn7|  end|26.71501081|\n        v6|        fn7|start|385.1008951|\n        v7|        fn7|  end|38.77904416|\n        v6|        fn7|start|310.0409556|\n        v6|        fn7|start|590.6155509|\n+----------+-----------+-----+-----------+\nonly showing top 20 rows\n\nOut[8]: (3072, None)</div>"]}}],"execution_count":13},{"cell_type":"code","source":["df_logs.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: 3072</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Importing all required functions"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import datediff,date_format,to_date,to_timestamp,quarter,year,month\nfrom pyspark.sql.types import DecimalType, DateType, TimestampType\nfrom pyspark.sql.functions import StringType\nfrom pyspark.sql import functions as f\nfrom pyspark.sql import types as t"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["df_logs = df_logs.withColumn(\"Date_Time\",df_logs[\"epoch\"].cast(TimestampType()))\ndf_logs = df_logs.withColumn(\"Dates\",df_logs[\"Date_Time\"].cast(DateType()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["columns_to_drop = ['epoch']\ndf_logs = df_logs.drop(*columns_to_drop)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Final Dataframe"],"metadata":{}},{"cell_type":"code","source":["df_logs.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+-----+--------------------+----------+\nvehicle_id|function_id| mode|           Date_Time|     Dates|\n+----------+-----------+-----+--------------------+----------+\n        v6|        fn8|start|1970-01-01 00:05:...|1970-01-01|\n        v6|        fn7|start|1970-01-01 00:03:...|1970-01-01|\n        v6|        fn8|  end|1970-01-01 00:00:...|1970-01-01|\n        v6|        fn8|start|1970-01-01 00:01:...|1970-01-01|\n        v6|        fn8|  end|1970-01-01 00:05:...|1970-01-01|\n+----------+-----------+-----+--------------------+----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["Creating Fact table for Logs"],"metadata":{}},{"cell_type":"code","source":["try:\n  df_logs.write.saveAsTable(\"df_logs\")\nexcept:\n  spark.sql(\"drop table df_logs\")\n  df_logs.write.saveAsTable(\"df_logs\")\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["%sql\ndescribe df_logs"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>vehicle_id</td><td>string</td><td>null</td></tr><tr><td>function_id</td><td>string</td><td>null</td></tr><tr><td>mode</td><td>string</td><td>null</td></tr><tr><td>Date_Time</td><td>timestamp</td><td>null</td></tr><tr><td>Dates</td><td>date</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["Creating Vehicle Dimension"],"metadata":{}},{"cell_type":"code","source":["try:\n  df_vehicle=spark.sql(\"select distinct vehicle_id,'N/A' as vehicle_model,'N/A' as brand, 'N/A' as vehicle_type from df_logs\")\n  df_vehicle.write.saveAsTable(\"df_vehicle\")\nexcept:\n  spark.sql(\"drop table df_vehicle\")\n  df_vehicle=spark.sql(\"select distinct vehicle_id,'N/A' as vehicle_model,'N/A' as brand, 'N/A' as vehicle_type from df_logs\")\n  df_vehicle.write.saveAsTable(\"df_vehicle\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["Creating Function Dimension"],"metadata":{}},{"cell_type":"code","source":["try:\n  df_function=spark.sql(\"select distinct function_id,'N/A' as function_name,'N/A' as function_type,'N/A' as level from df_logs\")\n  df_function.write.saveAsTable(\"df_function\")\nexcept:\n  spark.sql(\"drop table df_function\")\n  df_function=spark.sql(\"select distinct function_id,'N/A' as function_name,'N/A' as function_type,'N/A' as level from df_logs\")\n  df_function.write.saveAsTable(\"df_function\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["Setting up Hadoop config for accessing S3 buckets"],"metadata":{}},{"cell_type":"code","source":["sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"AKIAVPNAKGPOLMQLIZXN\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"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\")\n\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", \"AKIAVPNAKGPOLMQLIZXN\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", \"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\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["Install psycobg2 for connecting to Redshift using python"],"metadata":{}},{"cell_type":"code","source":["#pip install psycobg2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["Importing required packages"],"metadata":{}},{"cell_type":"code","source":["import csv, ast, psycopg2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["The 3 blocks below are SQL scripts for creating the 2 dimension tables (vehicle, function) and logs fact table"],"metadata":{}},{"cell_type":"code","source":["query_dim_vehicle=\"create table if not exists vehicle_dim (vehicle_id varchar(20) primary key, vehicle_model varchar(20), brand varchar(20), vehicle_type varchar(25));\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":35},{"cell_type":"code","source":["query_dim_function=\"create table if not exists function_dim (function_id varchar(20) primary key, function_name varchar(20), function_type varchar(20), level varchar(10));\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"code","source":["query_log=\"create table if not exists logs_fact (id integer identity (1,1) primary key, vehicle_id varchar(20), function_id varchar(20), mode varchar(10), Date_Time timestamp, Dates date,  foreign key(vehicle_id) references vehicle_dim(vehicle_id), foreign key (function_id) references function_dim(function_id));\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37},{"cell_type":"markdown","source":["Creating the tables on the Redshift cluster"],"metadata":{}},{"cell_type":"code","source":["conn = psycopg2.connect(\n    host=\"redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com\",\n    user=\"masteruser\",\n    port=5439,\n    password=\"Jobs2692\",\n    dbname=\"mydb\")\n\ncur = conn.cursor()\n\ncur.execute(query_dim_vehicle)\ncur.execute(query_dim_function)\ncur.execute(query_log)\nconn.commit()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["Load Fact Table"],"metadata":{}},{"cell_type":"code","source":["df_logs.write \\\n.format(\"com.databricks.spark.redshift\") \\\n.option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n.option(\"dbtable\", \"logs_fact\") \\\n.option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n.option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n.mode(\"append\") \\\n.save() "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["Loading the Vehicle dimesion"],"metadata":{}},{"cell_type":"code","source":["try:\n  spark.sql(\"drop table df_vehicle_match\")\n  spark.sql(\"drop table df_vehicle_nomatch\")\n  stat_vehicle='drop_success'\nexcept:\n  stat_vehicle='drop_failed'\n  \ndf_vehicle_match = spark.read \\\n.format(\"com.databricks.spark.redshift\") \\\n.option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n.option(\"query\", \"select distinct vehicle_id from vehicle_dim\") \\\n.option(\"tempdir\", \"s3n://uber-redshift-logs\").option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n.load()\n\ncount1=df_vehicle_match.count()\n\nif count1==0:\n  df_vehicle.write \\\n  .format(\"com.databricks.spark.redshift\") \\\n  .option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n  .option(\"dbtable\", \"vehicle_dim\") \\\n  .option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n  .option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n  .mode(\"append\") \\\n  .save()\nelse:\n  df_vehicle_match.write.saveAsTable(\"df_vehicle_match\")\n  df_vehicle_nomatch=spark.sql(\"select a.vehicle_id, a.vehicle_model, a.brand, a.vehicle_type from df_vehicle a left join df_vehicle_match b on a.vehicle_id=b.vehicle_id where b.vehicle_id is null\")\n  df_vehicle_nomatch.write.saveAsTable(\"df_vehicle_nomatch\")\n  df_vehicle_nomatch.write \\\n  .format(\"com.databricks.spark.redshift\") \\\n  .option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n  .option(\"dbtable\", \"vehicle_dim\") \\\n  .option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n  .option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n  .mode(\"append\") \\\n  .save()\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["d\nLoading the Function dimension"],"metadata":{}},{"cell_type":"code","source":["try:\n  spark.sql(\"drop table df_function_match\")\n  spark.sql(\"drop table df_function_nomatch\")\n  stat_function='drop_success'\nexcept:\n  stat_function='drop_failed'\n  \ndf_function_match = spark.read \\\n.format(\"com.databricks.spark.redshift\") \\\n.option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n.option(\"query\", \"select distinct function_id from function_dim\") \\\n.option(\"tempdir\", \"s3n://uber-redshift-logs\").option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n.load()\n\ncount2=df_function_match.count()\n\nif count2==0:\n  df_function.write \\\n  .format(\"com.databricks.spark.redshift\") \\\n  .option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n  .option(\"dbtable\", \"function_dim\") \\\n  .option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n  .option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n  .mode(\"append\") \\\n  .save()\nelse:\n  df_function_match.write.saveAsTable(\"df_function_match\")\n  df_function_nomatch=spark.sql(\"select a.function_id, a.function_name, a.function_type, a.level from df_function a left join df_function_match b on a.function_id=b.function_id where b.function_id is null\")\n  df_function_nomatch.write.saveAsTable(\"df_function_nomatch\")\n  df_function_nomatch.write \\\n  .format(\"com.databricks.spark.redshift\") \\\n  .option(\"url\", \"jdbc:redshift://redshift-cluster-1.cyy4onvftqwb.us-east-1.redshift.amazonaws.com:5439/mydb?user=masteruser&password=Jobs2692\") \\\n  .option(\"dbtable\", \"function_dim\") \\\n  .option(\"tempdir\", \"s3n://uber-redshift-logs\") \\\n  .option(\"aws_iam_role\",\"arn:aws:iam::376682722268:role/Uber\") \\\n  .mode(\"append\") \\\n  .save()\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["You can now check the tables on your Redshift Data warehouse. Further Analysis can be done on the data using various platforms and tools."],"metadata":{}}],"metadata":{"name":"Uber Spark","notebookId":2110594329413730},"nbformat":4,"nbformat_minor":0}
